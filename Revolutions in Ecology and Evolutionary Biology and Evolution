---
title: "Revolutions in Ecology and Evolutionary Biology"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```
#### preamble
This reproduces the revolutions analysis pipeline given in On Revolutions. We are going to use our data set comprising every paper available on JSTOR. The script was written by Matthias Mauch and modified by Ben Lambert, Armand Leroi, Marina Papadopolou & Sam McKay. 
See blog post by Ted Underwood critiquing the orginal Mauch et al. 2015 method; this revised method incorporates Underwood's recommended permutation on the diagonal https://tedunderwood.com/2016/02/07/you-say-you-found-a-revolution/
install.packages("nloptr")
#### libraries
```{r}
rm(list=ls())
library(reshape2)
library(flexmix)
library(ggplot2)
library(plyr)
library(scales)
library(tidyr)
library(broom)
library(dplyr)
library(rstanarm)
library(rstan)
library(gplots)
library(philentropy)
library(textmineR)
library(data.table)
options(mc.cores=parallel::detectCores())
```
#### data
The data are LONG form.
```{r}
d<-fread("EEpaperslong.csv", header=TRUE)
```
If desired subset data to ecology or evolutionary biology
```{r}
#d1<-subset(d,  evolution_paper==1)
```
If desired subset data to topic use
````{r}
#d1<-subset(d1, topic_use==1 & topic_discipline=="evolutionary biology")  #& , 
`````
subset data to year range
````{r}
d1<-subset(d, year >=1900 & year <=2010)
`````
Call number of papers
````{r}
length(unique(d1$paper_id))
`````
Call number of topics
````{r}
length(unique(d1$originaltopic))

`````
Summarize data by topic and year
````{r}
d2 <- ddply(d1, .(topic_label, year), summarise, mean=mean(prob))
``````
Reshape data to wide format
```{r}
d3<-dcast(d2,year~topic_label, value.var="mean")
```
#### make distance matrix among years
The next step is to make a distance matrix between years. We are going to use Jensen-Shannon distance, a symmetrized version of KL divergence, and make a heatmap.  It comes in two versions: undifferenced and differenced.

##### Difference and standardise data
Rho calculation - Remove #s to run, will take a long time
```{r}
N <-nrow(d3)
K <-ncol(d3)
X<-d3
stan_data <- list(N=N, K=K, X=X)
model <- stan_model("Rho.stan")
#fit <- sampling(model, data=stan_data)
#fit

```

If Rho >0.25 difference data then standardize the frequencies of the data to account for missing topics.
```{r}
d4<-data.frame(diff(as.matrix(d3), differences=1))  #If Rho <0.25 d4<-d3
d4$year<-NULL
rwsums<-rowSums(d4)
d4<-d4/rwsums
rwsums<-rowSums(d4)
rwsums
`````
Get the distance matrix and then take the square root to get Jensen_shannon distance
```{r}
d5<-d4
d5$year<-NULL
d5<-as.matrix(d5)
d3.1<-d3[-c(1),] #If rho<0.25 then skip this step
year<-unique(d3.1$year) 
rownames(d5)<-year
d7<-t(d5)
d8<-KLdiv(as.matrix(d7))
d9<- d8
d9[lower.tri(d9)] <- 0
dm.ut<- d9
d9<- d8
d9[upper.tri(d9)] <- 0
dm.lt<- d9
dm.ut.s<- dm.ut + t(dm.ut)
dm.lt.s<- dm.lt + t(dm.lt)
dm.s<- dm.ut.s + dm.lt.s
dm.s<-sqrt(dm.s)
dm<-dm.s
````
Heatmap of distances between years
````{r}
dmh<-melt(dm)
names(dmh)<-c("year1", "year2", "distance")
hplot<-ggplot()+
geom_tile(data=dmh, aes(x=year1,y=year2,fill=distance))+
scale_fill_continuous(low="lightgoldenrod1", high="coral3")+
ylab("year")+
xlab("year")+
  ggtitle("Distances for All Papers Between Years")+
guides(colour=FALSE, fill=FALSE)+
scale_x_continuous(breaks=pretty_breaks(n=5), limits=c(min(dmh$year1), max(dmh$year1)))+#
scale_y_continuous(breaks=pretty_breaks(n=5), limits=c(min(dmh$year2), max(dmh$year2)))+
theme_classic(base_size = 12, base_family = "sans")+
theme(axis.line.x = element_line(colour = 'black', size=0.5, linetype='solid'),axis.line.y = element_line(colour = 'black', size=0.5, linetype='solid'), legend.position="bottom", text=element_text(size=10,  family="sans", colour = 'black'))
hplot
#ggsave("Heatmap_of_distances_across_all_years.png")
``````
### Foote Novelty estimation
This is a single large function which takes the distance matrix, dm, and FN over all kernal widths, does the bootstrapping on the diagonals, and then gets the significance thresholds. 
```{r}
novelty.analysis <- function(dm, width.vector = c(1,2,3), ft.save.drcty = "foote.results.csv",
                             th.save.drcty = 'foote.thresh.csv', n.boot = 1000)
  {
# INITIALIZING: 
# initialize output dataframes
  thresholds <- data.frame(matrix(nrow = length(width.vector), ncol = 3))
  thresh.header <- c('hw', 'lower_2.5', 'upper_97.5')
  colnames(thresholds) <- thresh.header
  
  foote.results <- data.frame(matrix(nrow = (length(dm[1,])*length(width.vector)), ncol = 3))
  foote.header <- c('year', 'foote.novelty', 'hw')
  colnames(foote.results) <- foote.header
#count years
  years <-  length(rownames(dm))
  yearnames <- rownames(dm)
# DEFINING FUNCTIONS : 
# 1. make the kernal
  make.foote.kernel <- function(half.width, taper.width.factor=0.4, middle.bit=1) {
# Make the Foote kernel
# parameters:
# taper.width.factor. Width of the Gaussian tapering (default 0.4; 0 = no tapering)
#middle.bit size of the center (default: 1, as in DoP paper, Foote uses 0)
  ones <- mat.or.vec(half.width, half.width) + 1
  short.strip <- mat.or.vec(half.width,middle.bit)
  top <- cbind(-ones,short.strip,ones)
  long.strip <- mat.or.vec(middle.bit,2*half.width+middle.bit)
  kernel <- rbind(top,long.strip,-top)
  if (taper.width.factor != 0) {
    gaussian <- dnorm(1:(2*half.width+middle.bit),
                      half.width+0.5+0.5*middle.bit,
                      2*taper.width.factor*half.width)
    kernel <- sweep(kernel,2,gaussian,'*')
    kernel <- sweep(kernel,1,gaussian,'*')
  }
  return(kernel)
  }
#so this makes the kernal, which then gets put into the calculate.foote.novelty function, below, along with dm
# 2. calculate FN
  calculate.foote.novelty <- function(dm, kernel) {
# Calculate the Foote novelty given a distance matrix dm and the Foote kernel
   n.date <- nrow(dm)
   kernel.width <- ncol(kernel)
   novelty <- mat.or.vec(n.date,1) * NA
   n.step <- n.date - kernel.width
   for (i in 1:n.step) {
    ind <- i-1+(1:kernel.width)
    novelty[i+ceiling(kernel.width/2)] <- sum(dm[ind,ind] * kernel)
   }
   return(novelty)
}
## 3.  bootstrap
  diag.mm <- function(mat, offset=0, in.diag=c()) {
    n <- dim(mat)[1]
    m <- n-abs(offset)
    if (length(in.diag) == 0) {
      out.diag <- mat.or.vec(m,1)
      for (i in 1:m) {
        out.diag[i] <- mat[i, i+offset]
      }
      return(out.diag)
    }
    else {
      for (i in 1:m) {
        mat[i, i+offset] <- in.diag[i]
      }
      return(mat)
    }
  }
  shuffle.diagonals <- function(mat, symm=TRUE) {
    n <- dim(mat)[1]
    for (i in 0:(n-1)) {
      the.diag <- diag.mm(mat, offset=i)
      shuffled.diag <- sample(the.diag)
      mat <- diag.mm(mat, offset=i, in.diag=shuffled.diag)
    }
    mat[lower.tri(mat)] <- 0
    mat <- mat + t(mat) - diag(diag(mat))
    return(mat)
  }
  shuffled.bootstrap <- function(dm, kernel, n.boot) {
    n.boot <- 100
    foote.novelty <- calculate.foote.novelty(dm, kernel)
    n.nov <- length(foote.novelty)
    foote.novelty.scrambled <- mat.or.vec(n.boot, n.nov)
    for (i.boot in 1:n.boot) {
      shuffled.dm <- shuffle.diagonals(dm)
      foote.novelty.scrambled[i.boot,] <- calculate.foote.novelty(shuffled.dm, kernel)
    }
    thresh <- quantile(foote.novelty.scrambled, c(0.025,0.975), na.rm = T)
    return(thresh)
  }
## 4.  bind previous together, run, and return results in list
  run.foot.novelty <- function(distance.matrix, half.width,  n.boot = 1000){
# results list to return:
    return.list <- list()
# basic novelty
    kernel <- make.foote.kernel(half.width = half.width) 
    foote.novelty <- calculate.foote.novelty(distance.matrix, kernel)
    thresh <- shuffled.bootstrap(distance.matrix, kernel, n.boot = n.boot ) # choose your number of bootstrap iterations
    return.list$Thresh <-thresh
    
# a simple plot of novelty including very simple 95% confidence thresholds
    
    #print(plot(foote.novelty,type='b', main = paste('Novelty with half.width', half.width)))
    #print(abline(h=thresh))
# sort data for writing
    foote.res<-as.data.frame(foote.novelty)
    foote.res$year <- rownames(distance.matrix)
# add results to return list
    return.list$Foote.res <- foote.res
    return(return.list)
  }

# RUNNING FOOTE NOVELTY FOR DIFFERENT WIDTHS
# run through width vector
  c <- 1
  y <- 1
  
  for (i in width.vector){
    foote.results[y:(y+years-1),3] <- i
    new.nov <- run.foot.novelty(dm, half.width = i , n.boot = n.boot)
    thresholds[c,] <- c(i, new.nov$Thresh)
#foote.header<-c(foote.header, as.character(i))
    foote.results[y:(y+years-1),2] <- new.nov$Foote.res[,1]
    foote.results[y:(y+years-1),1] <- yearnames
    c <- c+1
    y <- y + years
  }
#export CSVs
write.csv(foote.results, 'foote_results_All_Papers.csv' , row.names=FALSE)
write.csv(thresholds, 'thresholds_All_Papers.csv', row.names=FALSE)
#if we want to return the results that we saved in the csv file
#return(return.list)
list.to.return <- list()
  list.to.return$Foote.res <- foote.results
  list.to.return$Thresh <- thresholds
  return(list.to.return)
}
```
#### Run the FN calculator. 
We are going to do this for all kernal widths that the data alow, so we need to calculate that.
```{r}
#calculate max_k, and use as upperlimit on k
max_k<-floor((length(unique(rownames(dm)))-2)/2)
#Set the range of kernal half-widths you want to use, we are going to use all allowed 
foote.results<-novelty.analysis(dm, c(1:max_k))
```

#### FN plot 
With revolutions in red, conservative periods in blue, and FN values in shades of grey
```{r}
t<-foote.results$Thresh
td<-as.data.frame(t)
f<-foote.results$Foote.res
fd<-as.data.frame(f)
res<-merge(fd, t, by.x="hw", by.y="hw")
res$year<-as.numeric(as.character(res$year))
res$foote.novelty<-as.numeric(as.character(res$foote.novelty))
res<-as.data.table(res)
res<-res %>% group_by(hw) %>% mutate(bin = cut(foote.novelty, breaks = 30,labels=1:30, include.lowest = TRUE))
res<-as.data.frame(res)
res$bin<-as.factor(res$bin)
res2<-res
res2$revcons<-as.character(ifelse(res2$foote.novelty>=res2$upper, "rev", ifelse(res$foote.novelty<=res2$lower, "cons", "neither")))
res2$revcons[is.na(res2$revcons)] <- "NA"
res2$revcons<- factor(res2$revcons, levels = c("rev", "cons", "NA"))
res2<-as.data.frame(res2)
```
Make palettes
````{r}
greys<-floor(seq(from=90, to=1, length.out=30))
greys<-paste("grey", greys, sep="")
pal1<-c(greys)
pal2<-c("indianred3", "steelblue3", "white")
```
FN plot
```{r}
fnplot2<-ggplot()+
geom_tile(data=res, aes(x=year,y=hw,fill=as.factor(bin)))+
scale_fill_manual(values=c(pal1), na.value=NA)+
geom_point(data=res2, aes(x=year,y=hw, colour=as.factor(revcons)), size=2, alpha=1)+
scale_colour_manual(values=c(pal2))+
ylab("k")+
xlab("year")+
ggtitle("Foote Novelty plot of all papers")+
guides(colour=FALSE, fill=FALSE)+
#geom_rect(aes(xmin=1895, xmax=1905, ymin=0, ymax=20), fill="red", alpha=0.2)+ #revolution 1
#geom_rect(aes(xmin=1950, xmax=1994, ymin=0, ymax=59), fill="red", alpha=0.2)+ #revolution 1
scale_x_continuous(breaks=pretty_breaks(n=5), limits=c(min(res$year), max(res$year)))+#
scale_y_continuous(breaks=pretty_breaks(n=10), limits=c(0, max_k))+
theme_classic(base_size = 12, base_family = "sans")+
theme(axis.line.x = element_line(colour = 'black', size=0.5, linetype='solid'),axis.line.y = element_line(colour = 'black', size=0.5, linetype='solid'), legend.position="bottom", text=element_text(size=10,  family="sans", colour = 'black'))
#pdf(paste(d1, "FN_plot.pdf", sep="_")) # starts writing a PDF to file
plot(fnplot2, width=10, height=10) # makes the actual plot
#ggsave("FNPlot_all_papers.png")
dev.off()  
```

#### identify putative revolutions and conservative periods
Here we pull out those dates where FN is greater than the significance threshold for revolutions, and smaller than the signficance threshold for conservative periods
````{r}
revconsperiods<-subset(res2, revcons=="rev"|revcons=="cons")
revconsperiods<-revconsperiods[order(revconsperiods$year, revconsperiods$hw),]
revconsperiods<-revconsperiods[c("year","hw","revcons")]
revconsperiods
#write.csv(revconsperiods, paste(d1, "revs.csv", sep="_"), row.names=FALSE)
````

#### estimate the expected number of False Positives
We are carrying out MANY significance tests.  So, by we'll expect to get quite a few by chance alone.  To claim that there is a revolution, we want the observed number of significance tests to be greater than the number of tests observed by chance alone. To do this, we calculate the number of FN estimates that we've made, then how many of them might be signficant by chance along, and then how many of them were.
````{r}
FNcount<-length(res2$foote.novelty[!is.na(res2$foote.novelty)])
sig<-subset(res2, revcons=="rev")
sigFNcount<-length(sig$revcons)
````
This is the number of tests you did
````{r}
FNcount
````
This is the number tests expected to be significant by chance alone, given alpha=0.05 (two tailed)
````{r}
sigFNchance<-FNcount*0.05/2
sigFNchance
````
This is the number of significant tests observed
````{r}
sigFNcount  
````
If the number of significant tests observed is greater than the number expected by chance, then you have at least one revolution in the data.
````{r}
ifelse(sigFNcount>sigFNchance, "There is a revolution", "There is no revolution")  
````
#### make a total Foote Novelty index, R.
R is an index of the rate of change across all kernal.widths. The idea is that it tells us how much change is happening at any time, regardless of whether or not it is significant. It works like this:

For each year, the sum of FN values, over all half-widths, k, (called "hw" in the data frame), divided by the sum of the mean FN values, over all k. The complication is that the ks for which FN values are estimated vary among years. See "On Revolutions" for more.
```{r}
i1<-res[c(1:3)]
#get the mean FN for all hw			   
i2<-ddply(i1, .(hw), summarise,
meanFN=mean(foote.novelty, na.rm=TRUE))
plot(i2$hw, i2$meanFN) 
#merge with i1
i1<-merge(i1, i2, by.x="hw", by.y="hw")
#remove meanFNs from years in which they are not calculated
#i1$meanFN<-as.numeric(as.character(ifelse(is.na(i1$foote.novelty), "NA", i1$meanFN)))
i1$normFN<-i1$foote.novelty/i1$meanFN
#get the sums of the normalized FNs
i2<-ddply(i1, .(year), summarise,
			   R=mean(normFN, na.rm=TRUE),
        count = length(normFN))
````
Plot R.
````{r}
indexplot<-ggplot()+
geom_hline(yintercept=1, linetype="dotted", size=1, alpha=1)+
geom_vline(xintercept=c(40,50), colour="tomato4", alpha=1, size=1)+
geom_line(data=i2, aes(x=year,y=R), size=0.5, alpha=1)+
guides(colour=FALSE)+
ylab("R")+
xlab("year")+
  ggtitle("Foote Novelty index plot all papers") +
scale_x_continuous(breaks=pretty_breaks(n=5), limits=c(min(res$year), max(res$year)))+#
scale_y_continuous(breaks=pretty_breaks(n=5))+
scale_fill_identity()+
scale_colour_identity()+
theme_classic(base_size = 12, base_family = "sans")+
theme(axis.line.x = element_line(colour = 'black', size=0.5, linetype='solid'),axis.line.y = element_line(colour = 'black', size=0.5, linetype='solid'), legend.position="bottom", text=element_text(size=10,  family="sans", colour = 'black'))
#pdf(paste(d1, "R_plot.pdf", sep="_")) # starts writing a PDF to file
plot(indexplot, width=10, height=10) # makes the actual plot
````
